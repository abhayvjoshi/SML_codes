{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from os import listdir\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import lda\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 50000/50000 [48:40<00:00, 17.12it/s]\n"
     ]
    }
   ],
   "source": [
    "mypath='unsup/'\n",
    "def getMovieDat():\n",
    "    \n",
    "    #documents = [f for f in listdir(mypath) if f.startswith('data_')]\n",
    "    #documents = [os.listdir(mypath)]\n",
    "    documents = [f for f in listdir(mypath) if f.endswith('.txt')]\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    docs=[]\n",
    "    sent_corp=defaultdict(list)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    wrd_freq=Counter()\n",
    "    for doc in tqdm(documents):\n",
    "        with open(mypath+doc,'r', errors = 'ignore') as fp:\n",
    "            lines = ' '.join(fp.readlines())\n",
    "            sent = sent_tokenize(lines)\n",
    "            tmp = ''\n",
    "            for sen in sent:\n",
    "                tokens=tokenizer.tokenize(sen)\n",
    "                tagged = nltk.pos_tag(tokens)\n",
    "                nouns = [word for word,pos in tagged \\\n",
    "                         if (pos == 'NN' or pos == 'NNP' or pos == 'NNS'\\\n",
    "                             or pos == 'NNPS' or pos=='JJ')]\n",
    "                downcased = [x.lower() for x in nouns]\n",
    "                downcased = [lmtzr.lemmatize(x) for x in downcased]\n",
    "                downcased = [x for x in downcased if x not in stopwords.words('english')]\n",
    "                if len(downcased)>1:\n",
    "                    sent_corp[doc].append([set(downcased)]+[sen])\n",
    "                    tmp += ' '.join(downcased)\n",
    "                    for x in downcased: wrd_freq[x]+=1\n",
    "            docs.append(tmp)\n",
    "    tp_wrds=sorted(wrd_freq.items(), key=itemgetter(1), reverse=True)\n",
    "    return (docs,tp_wrds,sent_corp)\n",
    "docs,tp_wrds,sent_corp = getMovieDat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( docs, open( \"docs.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = pickle.load( open( \"docs.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great majority film saydozen major silents crowd good last command city light latter chaplin circa br br apprehensive humor difficult uh enjoy decadelead actor film br intriguing sequenceguy minute schtickbackground dozen men naked white black wwibutt part full backsidewa early variation beefcake courtesy howard hughes'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block converts words to integers for LDA input\n",
    "def getMovNAmazonDat(docs):\n",
    "     \n",
    "    # change the location of data here\n",
    "    vectorizer = CountVectorizer(min_df=1,stop_words='english')\n",
    "    reditposts=[d for d in docs]\n",
    " \n",
    "    rv_vec=vectorizer.fit_transform(reditposts)\n",
    "    f_nam=tuple(vectorizer.get_feature_names())     \n",
    "    return(rv_vec,f_nam,docs)\n",
    "\n",
    "train_d,f_nam,docs = getMovNAmazonDat(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 50000\n",
      "INFO:lda:vocab_size: 399380\n",
      "INFO:lda:n_words: 3467369\n",
      "INFO:lda:n_topics: 500\n",
      "INFO:lda:n_iter: 1000\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "C:\\Anaconda\\lib\\site-packages\\lda\\utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:lda:<0> log likelihood: -60726348\n",
      "INFO:lda:<10> log likelihood: -45430808\n",
      "INFO:lda:<20> log likelihood: -42909913\n",
      "INFO:lda:<30> log likelihood: -41549608\n",
      "INFO:lda:<40> log likelihood: -40845687\n",
      "INFO:lda:<50> log likelihood: -40453785\n",
      "INFO:lda:<60> log likelihood: -40196098\n",
      "INFO:lda:<70> log likelihood: -40015332\n",
      "INFO:lda:<80> log likelihood: -39884683\n",
      "INFO:lda:<90> log likelihood: -39794660\n",
      "INFO:lda:<100> log likelihood: -39705187\n",
      "INFO:lda:<110> log likelihood: -39648534\n",
      "INFO:lda:<120> log likelihood: -39597428\n",
      "INFO:lda:<130> log likelihood: -39552747\n",
      "INFO:lda:<140> log likelihood: -39527431\n",
      "INFO:lda:<150> log likelihood: -39485421\n",
      "INFO:lda:<160> log likelihood: -39455572\n",
      "INFO:lda:<170> log likelihood: -39434822\n",
      "INFO:lda:<180> log likelihood: -39414189\n",
      "INFO:lda:<190> log likelihood: -39395681\n",
      "INFO:lda:<200> log likelihood: -39383336\n",
      "INFO:lda:<210> log likelihood: -39365613\n",
      "INFO:lda:<220> log likelihood: -39346688\n",
      "INFO:lda:<230> log likelihood: -39347018\n",
      "INFO:lda:<240> log likelihood: -39331232\n",
      "INFO:lda:<250> log likelihood: -39333537\n",
      "INFO:lda:<260> log likelihood: -39314703\n",
      "INFO:lda:<270> log likelihood: -39299411\n",
      "INFO:lda:<280> log likelihood: -39293507\n",
      "INFO:lda:<290> log likelihood: -39286726\n",
      "INFO:lda:<300> log likelihood: -39286711\n",
      "INFO:lda:<310> log likelihood: -39276194\n",
      "INFO:lda:<320> log likelihood: -39277808\n",
      "INFO:lda:<330> log likelihood: -39269218\n",
      "INFO:lda:<340> log likelihood: -39253220\n",
      "INFO:lda:<350> log likelihood: -39260583\n",
      "INFO:lda:<360> log likelihood: -39245886\n",
      "INFO:lda:<370> log likelihood: -39244992\n",
      "INFO:lda:<380> log likelihood: -39237262\n",
      "INFO:lda:<390> log likelihood: -39232439\n",
      "INFO:lda:<400> log likelihood: -39232355\n",
      "INFO:lda:<410> log likelihood: -39240458\n",
      "INFO:lda:<420> log likelihood: -39224250\n",
      "INFO:lda:<430> log likelihood: -39227118\n",
      "INFO:lda:<440> log likelihood: -39224444\n",
      "INFO:lda:<450> log likelihood: -39215157\n",
      "INFO:lda:<460> log likelihood: -39212865\n",
      "INFO:lda:<470> log likelihood: -39206596\n",
      "INFO:lda:<480> log likelihood: -39204445\n",
      "INFO:lda:<490> log likelihood: -39210190\n",
      "INFO:lda:<500> log likelihood: -39206660\n",
      "INFO:lda:<510> log likelihood: -39201002\n",
      "INFO:lda:<520> log likelihood: -39199555\n",
      "INFO:lda:<530> log likelihood: -39198392\n",
      "INFO:lda:<540> log likelihood: -39190426\n",
      "INFO:lda:<550> log likelihood: -39187931\n",
      "INFO:lda:<560> log likelihood: -39201889\n",
      "INFO:lda:<570> log likelihood: -39190209\n",
      "INFO:lda:<580> log likelihood: -39187111\n",
      "INFO:lda:<590> log likelihood: -39176401\n",
      "INFO:lda:<600> log likelihood: -39175181\n",
      "INFO:lda:<610> log likelihood: -39179429\n",
      "INFO:lda:<620> log likelihood: -39171003\n",
      "INFO:lda:<630> log likelihood: -39178211\n",
      "INFO:lda:<640> log likelihood: -39170964\n",
      "INFO:lda:<650> log likelihood: -39173421\n",
      "INFO:lda:<660> log likelihood: -39169867\n",
      "INFO:lda:<670> log likelihood: -39168422\n",
      "INFO:lda:<680> log likelihood: -39167987\n",
      "INFO:lda:<690> log likelihood: -39169048\n",
      "INFO:lda:<700> log likelihood: -39168365\n",
      "INFO:lda:<710> log likelihood: -39160170\n",
      "INFO:lda:<720> log likelihood: -39151442\n",
      "INFO:lda:<730> log likelihood: -39159935\n",
      "INFO:lda:<740> log likelihood: -39160485\n",
      "INFO:lda:<750> log likelihood: -39164500\n",
      "INFO:lda:<760> log likelihood: -39162280\n",
      "INFO:lda:<770> log likelihood: -39153515\n",
      "INFO:lda:<780> log likelihood: -39158673\n",
      "INFO:lda:<790> log likelihood: -39150553\n",
      "INFO:lda:<800> log likelihood: -39153344\n",
      "INFO:lda:<810> log likelihood: -39151290\n",
      "INFO:lda:<820> log likelihood: -39154928\n",
      "INFO:lda:<830> log likelihood: -39150777\n",
      "INFO:lda:<840> log likelihood: -39142746\n",
      "INFO:lda:<850> log likelihood: -39144141\n",
      "INFO:lda:<860> log likelihood: -39147124\n",
      "INFO:lda:<870> log likelihood: -39153932\n",
      "INFO:lda:<880> log likelihood: -39149285\n",
      "INFO:lda:<890> log likelihood: -39139742\n",
      "INFO:lda:<900> log likelihood: -39145058\n",
      "INFO:lda:<910> log likelihood: -39146655\n",
      "INFO:lda:<920> log likelihood: -39136822\n",
      "INFO:lda:<930> log likelihood: -39135088\n",
      "INFO:lda:<940> log likelihood: -39136802\n",
      "INFO:lda:<950> log likelihood: -39136405\n",
      "INFO:lda:<960> log likelihood: -39135603\n",
      "INFO:lda:<970> log likelihood: -39133213\n",
      "INFO:lda:<980> log likelihood: -39135960\n",
      "INFO:lda:<990> log likelihood: -39141731\n",
      "INFO:lda:<999> log likelihood: -39143511\n"
     ]
    }
   ],
   "source": [
    "# train the traditional LDA model\n",
    "model = lda.LDA(n_topics=500, n_iter=1000, random_state=1)\n",
    "model.fit(train_d)\n",
    "topic_word = model.topic_word_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( model, open( \"model.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load( open( \"model.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "topic_doc = pd.DataFrame(model.doc_topic_)\n",
    "maxprob = []\n",
    "maxprob = topic_doc.apply(lambda x: x.argmax(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_word = model.topic_word_\n",
    "doc_topic = model.doc_topic_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [02:42,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# qry=set(['screen','brightness','display','lcd','reflectiveness'])\n",
    "qry=set(['logan','apology','phone','parody','artist'])\n",
    "n_top_words = 20\n",
    "top_word_list = []\n",
    "for i, topic_dist in tqdm(enumerate(topic_word)):\n",
    "    topic_words = np.array(f_nam)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    tmp=set(topic_words)\n",
    "    #if qry.intersection(tmp):\n",
    "     #   print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "    top_word_list.append('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result=[]\n",
    "for i in maxprob:\n",
    "    result.append(top_word_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Topic 46: chaplin comedy little hardy film laurel tramp charlie early short stan circus trouble charley sequence silent gag ollie oliver studio'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( result, open( \"resultsml.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = \"I admit, the great majority of films released before say 1933 are just not for me. Of the dozen or so 'major' silents I have viewed, one I loved (The Crowd), and two were very good (The Last Command and City Lights, that latter Chaplin circa 1931).<br /><br />So I was apprehensive about this one, and humor is often difficult to appreciate (uh, enjoy) decades later. I did like the lead actors, but thought little of the film.<br /><br />One intriguing sequence. Early on, the guys are supposed to get 'de-loused' and for about three minutes, fully dressed, do some schtick. In the background, perhaps three dozen men pass by, all naked, white and black (WWI ?), and for most, their butts, part or full backside, are shown. Was this an early variation of beefcake courtesy of Howard Hughes?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I admit, the great majority of films released before say 1933 are just not for me. Of the dozen or so 'major' silents I have viewed, one I loved (The Crowd), and two were very good (The Last Command and City Lights, that latter Chaplin circa 1931).<br /><br />So I was apprehensive about this one, and humor is often difficult to appreciate (uh, enjoy) decades later. I did like the lead actors, but thought little of the film.<br /><br />One intriguing sequence. Early on, the guys are supposed to get 'de-loused' and for about three minutes, fully dressed, do some schtick. In the background, perhaps three dozen men pass by, all naked, white and black (WWI ?), and for most, their butts, part or full backside, are shown. Was this an early variation of beefcake courtesy of Howard Hughes?\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Topic 46: chaplin comedy little hardy film laurel tramp charlie early short stan circus trouble charley sequence silent gag ollie oliver studio'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
